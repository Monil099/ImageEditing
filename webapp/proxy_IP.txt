def get_free_proxies():
        url = "https://free-proxy-list.net/"
        # request and grab content
        soup = bs(requests.get(url).content, 'html.parser')
        # to store proxies
        proxies = []
        for row in soup.find("table", attrs={"class": "table-striped"}).find_all("tr")[1:]:
            tds = row.find_all("td")
            print(tds)
            try:
                ip = tds[0].text.strip()
                port = tds[1].text.strip()
                proxies.append(str(ip) + ":" + str(port))
            except IndexError:
                continue
        return proxies


    url = "https://www.oyorooms.com/allcities/"
    header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'}
    proxies = get_free_proxies()
    # print(proxies)

    for i in range(3):
    
        #printing req number
        print("Request Number : " + str(i+1))
        proxy = proxies[i]
        try:
            response = requests.get(url, headers=header, proxies = {"http":proxy, "https":proxy})
            print(response.json())
        except:
            # if the proxy Ip is pre_occupied
            print("Not Available")